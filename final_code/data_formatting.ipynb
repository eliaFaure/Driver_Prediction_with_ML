{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproccesing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "# Importing libraries\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.metrics import make_scorer, accuracy_score \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a14fc20-06cd-4fdb-a8f9-ce3bed23426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import copy\n",
    "import copy as py_copy\n",
    "\n",
    "#function for merging left and right dataframe\n",
    "def merge(left, right, name ):\n",
    "    pvs= pd.concat([left, right], axis=1)\n",
    "    length= len(left)\n",
    "\n",
    "    # Renaming labels\n",
    "    pvs.columns = [\n",
    "    'timestamp', 'acc_x_dashboard_l', 'acc_y_dashboard_l', 'acc_z_dashboard_l',\n",
    "    'acc_x_above_suspension_l', 'acc_y_above_suspension_l', 'acc_z_above_suspension_l', \n",
    "    'acc_x_below_suspension_l', 'acc_y_below_suspension_l', 'acc_z_below_suspension_l', \n",
    "    'gyro_x_dashboard_l', 'gyro_y_dashboard_l', 'gyro_z_dashboard_l', 'gyro_x_above_suspension_l', \n",
    "    'gyro_y_above_suspension_l', 'gyro_z_above_suspension_l', 'gyro_x_below_suspension_l', \n",
    "    'gyro_y_below_suspension_l', 'gyro_z_below_suspension_l', 'mag_x_dashboard_l', 'mag_y_dashboard_l', \n",
    "    'mag_z_dashboard_l', 'mag_x_above_suspension_l', 'mag_y_above_suspension_l', 'mag_z_above_suspension_l', \n",
    "    'temp_dashboard_l', 'temp_above_suspension_l', 'temp_below_suspension_l', 'timestamp_gps', \n",
    "    'latitude', 'longitude', 'speed', 'timestamp', 'acc_x_dashboard_r', 'acc_y_dashboard_r', \n",
    "    'acc_z_dashboard_r', 'acc_x_above_suspension_r', 'acc_y_above_suspension_r', \n",
    "    'acc_z_above_suspension_r', 'acc_x_below_suspension_r', 'acc_y_below_suspension_r', \n",
    "    'acc_z_below_suspension_r', 'gyro_x_dashboard_r', 'gyro_y_dashboard_r', 'gyro_z_dashboard_r', \n",
    "    'gyro_x_above_suspension_r', 'gyro_y_above_suspension_r', 'gyro_z_above_suspension_r', \n",
    "    'gyro_x_below_suspension_r', 'gyro_y_below_suspension_r', 'gyro_z_below_suspension_r', \n",
    "    'mag_x_dashboard_r', 'mag_y_dashboard_r', 'mag_z_dashboard_r', 'mag_x_above_suspension_r', \n",
    "    'mag_y_above_suspension_r', 'mag_z_above_suspension_r', 'temp_dashboard_r', 'temp_above_suspension_r', \n",
    "    'temp_below_suspension_r', 'timestamp_gps', 'latitude', 'longitude', 'speed'\n",
    "    ]\n",
    "    \n",
    "\n",
    "    # Remove duplicate columns (those with the same name, e.g., 'timestamp_gps', 'latitude', etc.)\n",
    "    pvs_removed = pvs.loc[:, ~pvs.columns.duplicated()]\n",
    "\n",
    "    # Columns to keep. We try these first\n",
    "    to_keep=[\"timestamp\",\n",
    "             \"acc_x_dashboard_l\",\n",
    "             \"acc_y_dashboard_l\",\n",
    "             \"acc_z_dashboard_l\",\n",
    "             \"speed\",\n",
    "             \"gyro_x_dashboard_l\",\n",
    "             \"gyro_y_dashboard_l\",\n",
    "             \"gyro_z_dashboard_l\"\n",
    "            ]\n",
    "    \n",
    "    pvs_removed=pvs_removed[to_keep]\n",
    "\n",
    "    #create the driver column ( target )\n",
    "    if name in (\"pvs1_gps_mpu\",\"pvs2_gps_mpu\",\"pvs3_gps_mpu\"):\n",
    "        pvs_removed['Driver'] = 1\n",
    "    elif name in (\"pvs4_gps_mpu\",\"pvs5_gps_mpu\",\"pvs6_gps_mpu\"):\n",
    "        pvs_removed['Driver'] = 2\n",
    "    else: \n",
    "        pvs_removed['Driver'] = 3\n",
    "        \n",
    "    return pvs_removed\n",
    "\n",
    "#Drivers do not drive in the start and end of each route. Function for removing these indicies\n",
    "def remove_zero_values(dfs):\n",
    "    new = py_copy.deepcopy(dfs) \n",
    "    non_zero_indices = {}\n",
    "    threshold = 0.1  # Define the threshold to detect non-zero speeds\n",
    "\n",
    "    for key, df in new.items():\n",
    "\n",
    "        first_non_zero_index = (df['speed'].abs() > threshold).idxmax()\n",
    "        last_non_zero_index = (df['speed'][::-1].abs() > threshold).idxmax()\n",
    "\n",
    "        non_zero_indices[key] = (first_non_zero_index, last_non_zero_index)\n",
    "\n",
    "        \n",
    "        # Plot the values of speed at the beginning of the dataframe\n",
    "        # plt.figure(figsize=(10, 5))\n",
    "        # plt.plot(df['speed'].iloc[first_non_zero_index:last_non_zero_index], label='Speed at Beginning')\n",
    "        # plt.xlabel('Index')\n",
    "        # plt.ylabel('Speed')\n",
    "        # plt.title(f'Speed at Beginning for {key}')\n",
    "        # plt.legend()\n",
    "        # plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "    # Iterate through each dataset and its respective non-zero index\n",
    "    for key, index in non_zero_indices.items():\n",
    "        new[key] = new[key].iloc[index[0]:index[1]].reset_index(drop=True)  # Remove rows up to the index and reset index\n",
    "\n",
    "    return new\n",
    "\n",
    "#Function for making dfs equal size in order to avoid bias. Each route is truncated to the same length\n",
    "def equalize_dfs(dfs):\n",
    "\n",
    "    new = dfs\n",
    "\n",
    "    # Split DataFrames into three routes\n",
    "    dfs_keys = list(new.keys())  # Get all keys in the dictionary\n",
    "    first_route_keys = [dfs_keys[i] for i in [0, 3, 6]]  # First three keys for the first route\n",
    "    second_route_keys = [dfs_keys[i] for i in [1, 4, 7]]  # Next three keys for the second route\n",
    "    third_route_keys = [dfs_keys[i] for i in [2, 5, 8]]  # Last three keys for the third route\n",
    "\n",
    "\n",
    "    # Calculate the minimum lengths for each route\n",
    "    min_length_first_route = min(len(new[key]) for key in first_route_keys)\n",
    "    max_length_first_route = max(len(new[key]) for key in first_route_keys)\n",
    "    min_length_second_route = min(len(new[key]) for key in second_route_keys)\n",
    "    max_length_second_route = max(len(new[key]) for key in second_route_keys)\n",
    "    min_length_third_route = min(len(new[key]) for key in third_route_keys)\n",
    "    max_length_third_route = max(len(new[key]) for key in third_route_keys)\n",
    "    \n",
    "\n",
    "    print(f\"removed length in the first route: {min_length_first_route-max_length_first_route}\")\n",
    "    \n",
    "    \n",
    "    print(f\"Shortest length in the second route: {min_length_second_route-max_length_second_route}\")\n",
    "    \n",
    "    print(f\"Shortest length in the third route: {min_length_third_route-max_length_third_route}\")\n",
    "    \n",
    "\n",
    "    # Truncate all DataFrames in each route to the respective minimum length\n",
    "    for key in first_route_keys:\n",
    "        new[key] = new[key].iloc[:min_length_first_route].reset_index(drop=True)\n",
    "\n",
    "    for key in second_route_keys:\n",
    "        new[key] = new[key].iloc[:min_length_second_route].reset_index(drop=True)\n",
    "\n",
    "    for key in third_route_keys:\n",
    "        new[key] = new[key].iloc[:min_length_third_route].reset_index(drop=True)\n",
    "\n",
    "    return new\n",
    "\n",
    "#Need to make the ratio between dfs and ratio to be an integer in order for dataframe reduction to work\n",
    "def round_dfs(dfs, ratio):\n",
    "\n",
    "    new = dfs\n",
    "\n",
    "    for key, df in new.items():\n",
    "\n",
    "        rounded_length = len(df) - (len(df) % ratio)\n",
    "\n",
    "        new[key] = df.iloc[:rounded_length].reset_index(drop=True)\n",
    "\n",
    "    return new\n",
    "\n",
    "\n",
    "def reduce_df(df, ratio, name):\n",
    "    new = pd.DataFrame()\n",
    "    print(f\"Reducing {name}\")\n",
    "    \n",
    "    # Iterate in steps of 'ratio' through the DataFrame\n",
    "    for i in range(0, len(df) - ratio + 1, ratio):\n",
    "        # Select the rows from i to i+ratio (inclusive of i, exclusive of i+ratio)\n",
    "        subset = df.iloc[i:i+ratio]\n",
    "        \n",
    "        # Calculate the mean for each column in the selected rows\n",
    "        merged_row = subset.mean(axis=0)\n",
    "        \n",
    "        # Append the result as a new row to the DataFrame `new`\n",
    "        new = pd.concat([new, merged_row.to_frame().T], ignore_index=True)\n",
    "        \n",
    "    return new\n",
    "\n",
    "\n",
    "def delete_timestamp(df):\n",
    "    to_remove=[\"timestamp_max\",\"timestamp_min\",\"timestamp_mean\",\"timestamp_STD\"]\n",
    "    new= df.drop(columns=to_remove, axis=1, errors='ignore')\n",
    "\n",
    "    return new\n",
    "\n",
    "def update_name(df):\n",
    "    new_column_names = [ 'timestamp_max','acc_x_dashboard_l_max', 'acc_y_dashboard_l_max', \n",
    "        'acc_z_dashboard_l_max', 'speed_max', 'gyro_x_dashboard_l_max', \n",
    "        'gyro_y_dashboard_l_max', 'gyro_z_dashboard_l_max', \n",
    "        'timestamp_min', 'acc_x_dashboard_l_min', 'acc_y_dashboard_l_min', \n",
    "        'acc_z_dashboard_l_min', 'speed_min', 'gyro_x_dashboard_l_min', \n",
    "        'gyro_y_dashboard_l_min', 'gyro_z_dashboard_l_min', \n",
    "        'timestamp_mean', 'acc_x_dashboard_l_mean', 'acc_y_dashboard_l_mean', \n",
    "        'acc_z_dashboard_l_mean', 'speed_mean', 'gyro_x_dashboard_l_mean', \n",
    "        'gyro_y_dashboard_l_mean', 'gyro_z_dashboard_l_mean', \n",
    "        'timestamp_STD_', 'acc_x_dashboard_l_STD_', 'acc_y_dashboard_l_STD_', \n",
    "        'acc_z_dashboard_l_STD_', 'speed_STD_', 'gyro_x_dashboard_l_STD_', \n",
    "        'gyro_y_dashboard_l_STD_', 'gyro_z_dashboard_l_STD_',\n",
    "         'acc_x_dashboard_l_jerk', 'acc_y_dashboard_l_jerk', \n",
    "        'acc_z_dashboard_l_jerk', 'speed_jerk', 'gyro_x_dashboard_l_jerk', \n",
    "        'gyro_y_dashboard_l_jerk', 'gyro_z_dashboard_l_jerk'\n",
    "    ]\n",
    "    df.columns = new_column_names\n",
    "    return df\n",
    "\n",
    "\n",
    "#function for computing the rolling window and the aggregation functions, \n",
    "#--> returns a dictionary containing the 9 dataframes winodwed (still separated)\n",
    "def computeWindow(name,df, windowSize=100, show=False):\n",
    "    if show:\n",
    "        print(\"--------------------------\")\n",
    "        print(f\"computing dataset {name}\")\n",
    "    \n",
    "    #define empty dataframe  \n",
    "\n",
    "    X= df.iloc[:, :-1]\n",
    "    Y= df.iloc[:len(df)-windowSize+1, -1]\n",
    "    windowed_df=pd.DataFrame()\n",
    "\n",
    "    #define starting and ending index\n",
    "    for start_idx in range(len(df)-windowSize+1):\n",
    "        end_idx=start_idx+windowSize\n",
    "\n",
    "        #extract rows belonging to the window\n",
    "        window=X.iloc[start_idx:end_idx]\n",
    "\n",
    "        #take beginning timestamp and ending timestamp\n",
    "        start_timestamp=window.iloc[0,0]\n",
    "        end_timestamp=window.iloc[-1,0]\n",
    "\n",
    "        \n",
    "        #keep only sensor data\n",
    "        sensor_data_window=window.iloc[:,1:]\n",
    "\n",
    "        #compute metrics for the specific window\n",
    "        max_values=window.max()\n",
    "        min_values=window.min()\n",
    "        mean_values=window.mean()\n",
    "        std_values=window.std()\n",
    "        jerk_values=(sensor_data_window.iloc[-1]-sensor_data_window.iloc[0])/(end_timestamp-start_timestamp)\n",
    "\n",
    "        #concate them (place side by side)\n",
    "        new_row=pd.concat([max_values,min_values,mean_values,std_values,jerk_values])\n",
    "        new_row = new_row.to_frame().T \n",
    "\n",
    "        windowed_df = pd.concat([windowed_df, new_row], ignore_index=True)\n",
    "\n",
    "        \n",
    "        if show and start_idx%10000==0:\n",
    "            print(start_idx)\n",
    "\n",
    "    #place aside again  X and Y\n",
    "    final=pd.concat([delete_timestamp(update_name(windowed_df)),Y], axis=1)  \n",
    "    return final\n",
    "\n",
    "def equalize_by_sampling_dfs(dfs_windowed):\n",
    "    new = dfs_windowed\n",
    "\n",
    "    # Split DataFrames into three routes\n",
    "    dfs_keys = list(new.keys())  # Get all keys in the dictionary\n",
    "    first_route_keys = [dfs_keys[i] for i in [0, 3, 6]]  # First three keys for the first route\n",
    "    second_route_keys = [dfs_keys[i] for i in [1, 4, 7]]  # Next three keys for the second route\n",
    "    third_route_keys = [dfs_keys[i] for i in [2, 5, 8]]  # Last three keys for the third route\n",
    "\n",
    "\n",
    "    # Calculate the minimum lengths for each route\n",
    "    min_length_first_route = min(len(new[key]) for key in first_route_keys)\n",
    "    max_length_first_route = max(len(new[key]) for key in first_route_keys)\n",
    "    min_length_second_route = min(len(new[key]) for key in second_route_keys)\n",
    "    max_length_second_route = max(len(new[key]) for key in second_route_keys)\n",
    "    min_length_third_route = min(len(new[key]) for key in third_route_keys)\n",
    "    max_length_third_route = max(len(new[key]) for key in third_route_keys)\n",
    "    \n",
    "\n",
    "    print(f\"removed length in the first route: {min_length_first_route-max_length_first_route}\")\n",
    "    print(f\"Shortest length in the second route: {min_length_second_route-max_length_second_route}\")\n",
    "    print(f\"Shortest length in the third route: {min_length_third_route-max_length_third_route}\")\n",
    "    \n",
    "\n",
    "    # Truncate all DataFrames in each route to the respective minimum length\n",
    "    for key in first_route_keys:\n",
    "        new[key] = new[key].sample(n=min_length_first_route, random_state=42).reset_index(drop=True)\n",
    "    for key in second_route_keys:\n",
    "        new[key] = new[key].sample(n=min_length_second_route, random_state=42).reset_index(drop=True)\n",
    "    for key in third_route_keys:\n",
    "        new[key] = new[key].sample(n=min_length_third_route, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    return new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing datasets\n",
    "filepaths_left={\n",
    "    \"pvs1_gps_mpu\" : r\"../archive/PVS 1/dataset_gps_mpu_left.csv\",\n",
    "    \"pvs2_gps_mpu\" : r\"../archive/PVS 2/dataset_gps_mpu_left.csv\",\n",
    "    \"pvs3_gps_mpu\" : r\"../archive/PVS 3/dataset_gps_mpu_left.csv\",\n",
    "    \"pvs4_gps_mpu\" : r\"../archive/PVS 4/dataset_gps_mpu_left.csv\",\n",
    "    \"pvs5_gps_mpu\" : r\"../archive/PVS 5/dataset_gps_mpu_left.csv\",\n",
    "    \"pvs6_gps_mpu\" : r\"../archive/PVS 6/dataset_gps_mpu_left.csv\",\n",
    "    \"pvs7_gps_mpu\" : r\"../archive/PVS 7/dataset_gps_mpu_left.csv\",\n",
    "    \"pvs8_gps_mpu\" : r\"../archive/PVS 8/dataset_gps_mpu_left.csv\",\n",
    "    \"pvs9_gps_mpu\" : r\"../archive/PVS 9/dataset_gps_mpu_left.csv\"\n",
    "}\n",
    "\n",
    "filepaths_right={\n",
    "    \"pvs1_gps_mpu\" : r\"../archive/PVS 1/dataset_gps_mpu_right.csv\",\n",
    "    \"pvs2_gps_mpu\" : r\"../archive/PVS 2/dataset_gps_mpu_right.csv\",\n",
    "    \"pvs3_gps_mpu\" : r\"../archive/PVS 3/dataset_gps_mpu_right.csv\",\n",
    "    \"pvs4_gps_mpu\" : r\"../archive/PVS 4/dataset_gps_mpu_right.csv\",\n",
    "    \"pvs5_gps_mpu\" : r\"../archive/PVS 5/dataset_gps_mpu_right.csv\",\n",
    "    \"pvs6_gps_mpu\" : r\"../archive/PVS 6/dataset_gps_mpu_right.csv\",\n",
    "    \"pvs7_gps_mpu\" : r\"../archive/PVS 7/dataset_gps_mpu_right.csv\",\n",
    "    \"pvs8_gps_mpu\" : r\"../archive/PVS 8/dataset_gps_mpu_right.csv\",\n",
    "    \"pvs9_gps_mpu\" : r\"../archive/PVS 9/dataset_gps_mpu_right.csv\"\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "# Left and right corresponds to sensordata from either side of the cars\n",
    "datasets_left = {name:pd.read_csv(path) for name,path in filepaths_left.items()}\n",
    "datasets_right = {name:pd.read_csv(path) for name,path in filepaths_right.items()}\n",
    "\n",
    "datasets = {name: merge(datasets_left[name],datasets_right[name], name) for name in datasets_right.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio=1\n",
    "\n",
    "datasets_rm_zeros = remove_zero_values(datasets)\n",
    "equalized_datasets = equalize_dfs(datasets_rm_zeros)\n",
    "\n",
    "if ratio!=1:\n",
    "    rounded_datasets = round_dfs(equalized_datasets, ratio)\n",
    "    datasets_reduced={name:reduce_df(rounded_datasets[name],ratio,name) for name in rounded_datasets.keys()}\n",
    "else:\n",
    "    rounded_datasets=datasets_rm_zeros\n",
    "    datasets_reduced=rounded_datasets\n",
    "\n",
    "# Save the dictionary with DataFrames\n",
    "\n",
    "directory = 'datasets'\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "file_path = os.path.join(directory, 'datasets_reduced.pkl')\n",
    "with open(file_path, \"wb\") as file:\n",
    "    pickle.dump(datasets_reduced, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute all windowed dfs in order to store them : \n",
    "window_sizes=[3000]\n",
    "for size in window_sizes:\n",
    "    dfs_windowed={name:computeWindow(name,datasets_reduced[name],size, True) for name in datasets_reduced.keys()}\n",
    "    \n",
    "    # Save the dictionary with DataFrames\n",
    "    with open(f\"dfs_windowed_{size}_rd{ratio}.pkl\", \"wb\") as file:\n",
    "        pickle.dump(dfs_windowed, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
