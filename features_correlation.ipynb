{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ec03935-e373-428f-a845-66a34c0219ad",
   "metadata": {},
   "source": [
    "## TRAIN THE MODEL AND EVALUATE IT \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5ec4a13-9fbc-4f6e-9c9d-ea61b2bcbe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.metrics import make_scorer, accuracy_score \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d65b83-5b4f-411f-9203-2c3ae02508f3",
   "metadata": {},
   "source": [
    "## DEFINE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a14fc20-06cd-4fdb-a8f9-ce3bed23426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for merging left and right dataframe\n",
    "def merge(left, right, name ):\n",
    "    pvs= pd.concat([left, right], axis=1)\n",
    "    length= len(left)\n",
    "\n",
    "    # Renaming labels\n",
    "    pvs.columns = [\n",
    "    'timestamp', 'acc_x_dashboard_l', 'acc_y_dashboard_l', 'acc_z_dashboard_l',\n",
    "    'acc_x_above_suspension_l', 'acc_y_above_suspension_l', 'acc_z_above_suspension_l', \n",
    "    'acc_x_below_suspension_l', 'acc_y_below_suspension_l', 'acc_z_below_suspension_l', \n",
    "    'gyro_x_dashboard_l', 'gyro_y_dashboard_l', 'gyro_z_dashboard_l', 'gyro_x_above_suspension_l', \n",
    "    'gyro_y_above_suspension_l', 'gyro_z_above_suspension_l', 'gyro_x_below_suspension_l', \n",
    "    'gyro_y_below_suspension_l', 'gyro_z_below_suspension_l', 'mag_x_dashboard_l', 'mag_y_dashboard_l', \n",
    "    'mag_z_dashboard_l', 'mag_x_above_suspension_l', 'mag_y_above_suspension_l', 'mag_z_above_suspension_l', \n",
    "    'temp_dashboard_l', 'temp_above_suspension_l', 'temp_below_suspension_l', 'timestamp_gps', \n",
    "    'latitude', 'longitude', 'speed', 'timestamp', 'acc_x_dashboard_r', 'acc_y_dashboard_r', \n",
    "    'acc_z_dashboard_r', 'acc_x_above_suspension_r', 'acc_y_above_suspension_r', \n",
    "    'acc_z_above_suspension_r', 'acc_x_below_suspension_r', 'acc_y_below_suspension_r', \n",
    "    'acc_z_below_suspension_r', 'gyro_x_dashboard_r', 'gyro_y_dashboard_r', 'gyro_z_dashboard_r', \n",
    "    'gyro_x_above_suspension_r', 'gyro_y_above_suspension_r', 'gyro_z_above_suspension_r', \n",
    "    'gyro_x_below_suspension_r', 'gyro_y_below_suspension_r', 'gyro_z_below_suspension_r', \n",
    "    'mag_x_dashboard_r', 'mag_y_dashboard_r', 'mag_z_dashboard_r', 'mag_x_above_suspension_r', \n",
    "    'mag_y_above_suspension_r', 'mag_z_above_suspension_r', 'temp_dashboard_r', 'temp_above_suspension_r', \n",
    "    'temp_below_suspension_r', 'timestamp_gps', 'latitude', 'longitude', 'speed'\n",
    "    ]\n",
    "    \n",
    "\n",
    "    # Remove duplicate columns (those with the same name, e.g., 'timestamp_gps', 'latitude', etc.)\n",
    "    pvs_removed = pvs.loc[:, ~pvs.columns.duplicated()]\n",
    "\n",
    "    # Columns to keep. We try these first\n",
    "    to_keep=[\"timestamp\",\n",
    "             \"acc_x_dashboard_l\",\n",
    "             \"acc_y_dashboard_l\",\n",
    "             \"acc_z_dashboard_l\",\n",
    "             \"speed\",\n",
    "             \"gyro_x_dashboard_l\",\n",
    "             \"gyro_y_dashboard_l\",\n",
    "             \"gyro_z_dashboard_l\"\n",
    "            ]\n",
    "    \n",
    "    pvs_removed=pvs_removed[to_keep]\n",
    "\n",
    "    #create the driver column ( target )\n",
    "    if name in (\"pvs1_gps_mpu\",\"pvs2_gps_mpu\",\"pvs3_gps_mpu\"):\n",
    "        pvs_removed['Driver'] = 1\n",
    "    elif name in (\"pvs4_gps_mpu\",\"pvs5_gps_mpu\",\"pvs6_gps_mpu\"):\n",
    "        pvs_removed['Driver'] = 2\n",
    "    else: \n",
    "        pvs_removed['Driver'] = 3\n",
    "        \n",
    "    return pvs_removed\n",
    "\n",
    "#Drivers do not drive in the start and end of each route. Function for removing these indicies\n",
    "def remove_zero_values(dfs):\n",
    "\n",
    "    new = dfs\n",
    "\n",
    "    non_zero_indices = {}\n",
    "    threshold = 0.1  # Define the threshold to detect non-zero speeds\n",
    "\n",
    "    for key, df in new.items():\n",
    "\n",
    "        first_non_zero_index = (df['speed'].abs() > threshold).idxmax()\n",
    "        last_non_zero_index = (df['speed'][::-1].abs() > threshold).idxmax()\n",
    "\n",
    "        non_zero_indices[key] = (first_non_zero_index, last_non_zero_index)\n",
    "\n",
    "    #print(non_zero_indices)\n",
    "\n",
    "    # Iterate through each dataset and its respective non-zero index\n",
    "    for key, index in non_zero_indices.items():\n",
    "        new[key] = new[key].iloc[index[0]:index[1]].reset_index(drop=True)  # Remove rows up to the index and reset index\n",
    "\n",
    "    return new\n",
    "\n",
    "#Function for making dfs equal size in order to avoid bias. Each route is truncated to the same length\n",
    "def equalize_dfs(dfs):\n",
    "\n",
    "    new = dfs\n",
    "\n",
    "    # Split DataFrames into three routes\n",
    "    dfs_keys = list(new.keys())  # Get all keys in the dictionary\n",
    "    first_route_keys = dfs_keys[:3]  # First three keys for the first route\n",
    "    second_route_keys = dfs_keys[3:6]  # Next three keys for the second route\n",
    "    third_route_keys = dfs_keys[6:]  # Last three keys for the third route\n",
    "\n",
    "    # Calculate the minimum lengths for each route\n",
    "    min_length_first_route = min(len(new[key]) for key in first_route_keys)\n",
    "    min_length_second_route = min(len(new[key]) for key in second_route_keys)\n",
    "    min_length_third_route = min(len(new[key]) for key in third_route_keys)\n",
    "\n",
    "    #print(f\"Shortest length in the first route: {min_length_first_route}\")\n",
    "    #print(f\"Shortest length in the second route: {min_length_second_route}\")\n",
    "    #print(f\"Shortest length in the third route: {min_length_third_route}\")\n",
    "\n",
    "    # Truncate all DataFrames in each route to the respective minimum length\n",
    "    for key in first_route_keys:\n",
    "        new[key] = new[key].iloc[:min_length_first_route].reset_index(drop=True)\n",
    "\n",
    "    for key in second_route_keys:\n",
    "        new[key] = new[key].iloc[:min_length_second_route].reset_index(drop=True)\n",
    "\n",
    "    for key in third_route_keys:\n",
    "        new[key] = new[key].iloc[:min_length_third_route].reset_index(drop=True)\n",
    "\n",
    "    return new\n",
    "\n",
    "#Need to make the ratio between dfs and ratio to be an integer in order for dataframe reduction to work\n",
    "def round_dfs(dfs, ratio):\n",
    "\n",
    "    new = dfs\n",
    "\n",
    "    for key, df in new.items():\n",
    "\n",
    "        rounded_length = len(df) - (len(df) % ratio)\n",
    "\n",
    "        new[key] = df.iloc[:rounded_length].reset_index(drop=True)\n",
    "\n",
    "    return new\n",
    "\n",
    "### POSSIBLY OUTDATED\n",
    "#reduce the number of rows of the dataset by a factor ratio\n",
    "# def reduce_df(df,ratio,name):\n",
    "#     new = pd.DataFrame()\n",
    "#     print(f\"reducing {name}\")\n",
    "#     for i in range(0, len(df) - 1, ratio):  # Step by 2 to get pairs\n",
    "\n",
    "#         #if i%10000==0:\n",
    "#             #print(i)\n",
    "            \n",
    "#         row1 = df.iloc[i]\n",
    "#         row2 = df.iloc[i + 1]\n",
    "#         # Calculate the mean of the pair (assuming numeric data)\n",
    "#         merged_row = (row1 + row2) / 2\n",
    "        \n",
    "#         # Append the result as a new row to `new`\n",
    "#         new = pd.concat([new, merged_row.to_frame().T], ignore_index=True)\n",
    "        \n",
    "#     return new\n",
    "\n",
    "def reduce_df(df, ratio, name):\n",
    "    new = pd.DataFrame()\n",
    "    print(f\"Reducing {name}\")\n",
    "    \n",
    "    # Iterate in steps of 'ratio' through the DataFrame\n",
    "    for i in range(0, len(df) - ratio + 1, ratio):\n",
    "        # Select the rows from i to i+ratio (inclusive of i, exclusive of i+ratio)\n",
    "        subset = df.iloc[i:i+ratio]\n",
    "        \n",
    "        # Calculate the mean for each column in the selected rows\n",
    "        merged_row = subset.mean(axis=0)\n",
    "        \n",
    "        # Append the result as a new row to the DataFrame `new`\n",
    "        new = pd.concat([new, merged_row.to_frame().T], ignore_index=True)\n",
    "        \n",
    "    return new\n",
    "\n",
    "\n",
    "#helping function for reduction\n",
    "\n",
    "def delete_timestamp(df):\n",
    "    to_remove=[\"timestamp_max\",\"timestamp_min\",\"timestamp_mean\",\"timestamp_STD\"]\n",
    "    new= df.drop(columns=to_remove, axis=1, errors='ignore')\n",
    "\n",
    "    return new\n",
    "\n",
    "def update_name(df):\n",
    "    new_column_names = [ 'timestamp_max','acc_x_dashboard_l_max', 'acc_y_dashboard_l_max', \n",
    "        'acc_z_dashboard_l_max', 'speed_max', 'gyro_x_dashboard_l_max', \n",
    "        'gyro_y_dashboard_l_max', 'gyro_z_dashboard_l_max', \n",
    "        'timestamp_min', 'acc_x_dashboard_l_min', 'acc_y_dashboard_l_min', \n",
    "        'acc_z_dashboard_l_min', 'speed_min', 'gyro_x_dashboard_l_min', \n",
    "        'gyro_y_dashboard_l_min', 'gyro_z_dashboard_l_min', \n",
    "        'timestamp_mean', 'acc_x_dashboard_l_mean', 'acc_y_dashboard_l_mean', \n",
    "        'acc_z_dashboard_l_mean', 'speed_mean', 'gyro_x_dashboard_l_mean', \n",
    "        'gyro_y_dashboard_l_mean', 'gyro_z_dashboard_l_mean', \n",
    "        'timestamp_STD_', 'acc_x_dashboard_l_STD_', 'acc_y_dashboard_l_STD_', \n",
    "        'acc_z_dashboard_l_STD_', 'speed_STD_', 'gyro_x_dashboard_l_STD_', \n",
    "        'gyro_y_dashboard_l_STD_', 'gyro_z_dashboard_l_STD_',\n",
    "         'acc_x_dashboard_l_jerk', 'acc_y_dashboard_l_jerk', \n",
    "        'acc_z_dashboard_l_jerk', 'speed_jerk', 'gyro_x_dashboard_l_jerk', \n",
    "        'gyro_y_dashboard_l_jerk', 'gyro_z_dashboard_l_jerk'\n",
    "    ]\n",
    "    df.columns = new_column_names\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#function for computing the rolling window and the aggregation functions, \n",
    "#--> returns a disctionary containing the 9 dataframes winodwed (still separated)\n",
    "def computeWindow(name,df, windowSize=100, show=False):\n",
    "    if show:\n",
    "        print(\"--------------------------\")\n",
    "        print(f\"computing dataset {name}\")\n",
    "    \n",
    "    #define empty dataframe  \n",
    "\n",
    "    X= df.iloc[:, :-1]\n",
    "    Y= df.iloc[:len(df)-windowSize+1, -1]\n",
    "    windowed_df=pd.DataFrame()\n",
    "\n",
    "    #define starting and ending index\n",
    "    for start_idx in range(len(df)-windowSize+1):\n",
    "        end_idx=start_idx+windowSize\n",
    "\n",
    "        #extract rows belonging to the window\n",
    "        window=X.iloc[start_idx:end_idx]\n",
    "\n",
    "        #take beginning timestamp and ending timestamp\n",
    "        start_timestamp=window.iloc[0,0]\n",
    "        end_timestamp=window.iloc[-1,0]\n",
    "\n",
    "        \n",
    "        #keep only sensor data\n",
    "        sensor_data_window=window.iloc[:,1:]\n",
    "\n",
    "        #compute metrics for the specific window\n",
    "        max_values=window.max()\n",
    "        min_values=window.min()\n",
    "        mean_values=window.mean()\n",
    "        std_values=window.std()\n",
    "        jerk_values=(sensor_data_window.iloc[-1]-sensor_data_window.iloc[0])/(end_timestamp-start_timestamp)\n",
    "\n",
    "        #concate them (place side by side)\n",
    "        new_row=pd.concat([max_values,min_values,mean_values,std_values,jerk_values])\n",
    "        new_row = new_row.to_frame().T \n",
    "\n",
    "        windowed_df = pd.concat([windowed_df, new_row], ignore_index=True)\n",
    "\n",
    "        \n",
    "        if show and start_idx%10000==0:\n",
    "            print(start_idx)\n",
    "\n",
    "    #place aside again  X and Y\n",
    "    final=pd.concat([delete_timestamp(update_name(windowed_df)),Y], axis=1)  \n",
    "    return final\n",
    "\n",
    "\n",
    "#define train_test splits, based on the track we define in test_track. \n",
    "#we use test_track for testing and the other two tracks for training\n",
    "#-->returns X_train,y_train,X_test,y_test as numpy arrays\n",
    "\n",
    "def train_test_my_split(dfs,test_track,numpy_conversion=True):\n",
    "    #take track three for testing and trcakk 1,2 for trainig\n",
    "    all_keys=list(dfs.keys())\n",
    "\n",
    "    if test_track==3:\n",
    "        train_indices=[0,1,3,4,6,7]\n",
    "        test_indices=[2,5,8]\n",
    "    elif test_track==2:\n",
    "        train_indices=[0,2,3,5,6,8]\n",
    "        test_indices=[1,4,7]\n",
    "    elif test_track==1:\n",
    "        train_indices=[1,2,4,5,7,8]\n",
    "        test_indices=[0,3,6]\n",
    "        \n",
    "\n",
    "    train_dfs = [dfs[all_keys[i]] for i in train_indices]\n",
    "    train_df = pd.concat(train_dfs, axis=0, ignore_index=True)\n",
    "\n",
    "    # Shuffle the training data\n",
    "    train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    test_dfs = [dfs[all_keys[i]] for i in test_indices]\n",
    "    test_df = pd.concat(test_dfs, axis=0, ignore_index=True)\n",
    "\n",
    "    \n",
    "    # Shuffle testing data\n",
    "    test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    if numpy_conversion:\n",
    "        #split features and targets\n",
    "        X_train=train_df.iloc[:,:-1].to_numpy()\n",
    "        y_train=train_df.iloc[:,-1].to_numpy()\n",
    "    \n",
    "        X_test=test_df.iloc[:,:-1].to_numpy()\n",
    "        y_test=test_df.iloc[:,-1].to_numpy()\n",
    "    else:\n",
    "        #split features and targets\n",
    "        X_train=train_df.iloc[:,:-1]\n",
    "        y_train=train_df.iloc[:,-1]\n",
    "    \n",
    "        X_test=test_df.iloc[:,:-1]\n",
    "        y_test=test_df.iloc[:,-1]\n",
    "        \n",
    "\n",
    "    return X_train,y_train,X_test,y_test\n",
    "\n",
    "#function for creating the model based on the parameter type\n",
    "#--> returns the model\n",
    "def create_model(type):\n",
    "    if type==\"RandomForest\":\n",
    "        return RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    elif type == \"SVM\":\n",
    "        return SVC(kernel=\"rbf\", C=1.0)\n",
    "    elif type == \"lr\":\n",
    "        return LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "#test the model on the TEST set, take as input the NON-WINDOWED datasets\n",
    "#-->returns the accuracy on the test set\n",
    "\n",
    "def test_model(X_train,y_train,X_test,y_test,model_type,test_track=3):\n",
    "    #create the  model \n",
    "    model=create_model(model_type)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Test set evaluation\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"Test set accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "\n",
    "# function for cross-fold evaluation, with num_folds folds, taken as a parameter\n",
    "#--> returns average accuracy for the specific hyperparameters configuration defined as input\n",
    "\n",
    "def evaluate_model(X_train,y_train,model_type,num_folds,test_track=3):\n",
    "    \n",
    "    #create the  model \n",
    "    model=create_model(model_type)\n",
    "\n",
    "    #APPLY CROSS-FOLDER EVALUATION\n",
    "\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_accuracies = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_ttrain, X_val = X_train[train_index], X_train[val_index] \n",
    "        y_ttrain, y_val = y_train[train_index], y_train[val_index]\n",
    "            \n",
    "        model.fit(X_ttrain, y_ttrain) \n",
    "        y_pred = model.predict(X_val) \n",
    "        \n",
    "        accuracy = accuracy_score(y_val, y_pred) \n",
    "        print((y_val != y_pred).sum())\n",
    "        print('accuracy:', accuracy)\n",
    "        fold_accuracies.append(accuracy)\n",
    "\n",
    "    average_accuracy = sum(fold_accuracies) / num_folds\n",
    "    print('average of fold',average_accuracy)\n",
    "\n",
    "    return average_accuracy\n",
    "\n",
    "#intermediate function, used for: \n",
    "#windowing based on the window size\n",
    "#-->returns X_train,y_train,X_test,y_test based on track defined in test_track\n",
    "\n",
    "\n",
    "def window_and_split(dfs,window_size,test_track=3):\n",
    "    dfs_windowed={name:computeWindow(name,dfs[name]) for name in dfs.keys()}\n",
    "\n",
    "    return train_test_my_split(dfs_windowed,test_track)\n",
    "\n",
    "#main function, takes hyperparameters options, model type and num_folds for k-fold\n",
    "#tries all configurations on the evaluation set\n",
    "#test the best configuration on the trainig set \n",
    "\n",
    "def tuning_and_evaluation(dfs,window_sizes,model_type,num_folds,test_track=3):\n",
    "\n",
    "    #initialize optimal results\n",
    "    best_accuracy=0\n",
    "    best_window_size=0\n",
    "\n",
    "\n",
    "    #store all the average accuracies with different hyperparameters inside an arrray\n",
    "    tot_accuracies=[]\n",
    "\n",
    "    for window_size in window_sizes:\n",
    "        print(f\"--------------------\")\n",
    "        print(f\"EVALUATE window_size: {window_size}\")\n",
    "        \n",
    "        X_train,y_train,X_test,y_test=window_and_split(datasets_reduced,window_size)\n",
    "        \n",
    "        accuracy=evaluate_model(X_train,y_train,model_type,num_folds,test_track)\n",
    "        tot_accuracies.append(accuracy)\n",
    "        \n",
    "        \n",
    "        #update optimal results if needed\n",
    "        if accuracy>best_accuracy:\n",
    "            best_accuracy=accuracy\n",
    "            best_window_size=window_size\n",
    "            \n",
    "            \n",
    "    print(f\"Best window size: {best_window_size} with accuracy: {best_accuracy}\")\n",
    "    print(\"test best model on TEST data\")\n",
    "    \n",
    "    X_train,y_train,X_test,y_test=window_and_split(datasets_reduced,best_window_size)\n",
    "    \n",
    "    test_model(X_train,y_train,X_test,y_test,model_type,test_track)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f201ad47-b736-48b0-ab29-72b77eae9e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function for correlation \n",
    "def compute_correlation(df,plot=False):\n",
    "\n",
    "    # Only compute using the numeric data types in the dataset\n",
    "    numeric_df = df.select_dtypes(include=['number'])\n",
    "    \n",
    "    #print(numeric_df.columns)\n",
    "    correlation_matrix=numeric_df.corr()\n",
    "\n",
    "    if not plot:\n",
    "        return correlation_matrix\n",
    "        \n",
    "    # Create the heatmap\n",
    "    plt.figure(figsize=(18, 14))  # Increase the size of the figure\n",
    "    \n",
    "    # Plot the heatmap with adjusted figure size\n",
    "    sns.heatmap(correlation_matrix, annot=False, fmt='.2f', cmap='coolwarm', linewidths=0.5,vmin=-1, vmax=1)\n",
    "    \n",
    "    # Rotate the tick labels for better readability and shrink font size\n",
    "    plt.xticks(rotation=90, fontsize=8)  # Rotate x-axis labels by 90 degrees and adjust font size\n",
    "    plt.yticks(rotation=0, fontsize=8)   # Keep y-axis labels horizontal and adjust font size\n",
    "    \n",
    "    # Add a title with smaller font size\n",
    "    plt.title(\"Correlation Matrix\", fontsize=12)\n",
    "    \n",
    "    # Show the plot with a tight layout\n",
    "    plt.tight_layout()  # Adjust layout to prevent clipping\n",
    "    plt.show()\n",
    "\n",
    "    return correlation_matrix\n",
    "\n",
    "def remove_highly_correlated(df,corr,threshold=0.9):\n",
    "    #take absoulute values\n",
    "    corr_matrix_abs=corr.abs()\n",
    "    \n",
    "    #take only upper diagonal \n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool), k=1)\n",
    "    \n",
    "    highly_correlated_features=np.where(corr_matrix_abs*mask>threshold)\n",
    "    \n",
    "    #print and store highly correlated features\n",
    "    \n",
    "    to_drop=set()\n",
    "    for i,j in zip(*highly_correlated_features):\n",
    "        print( f\"highly correlated pair: {corr_matrix_abs.columns[i]}-{corr_matrix_abs.columns[j]},(correlation: {corr_matrix_abs.iloc[i, j]:.2f})\")\n",
    "        to_drop.add(corr_matrix_abs.columns[j])\n",
    "        \n",
    "        #print(corr_matrix_abs.columns[j])\n",
    "    \n",
    "    #remove to_drop columns\n",
    "    cleaned_df=df.drop(columns=to_drop)\n",
    "    #print(f\"droopped columns {to_drop}\")\n",
    "    #print(f\"removed {count} columns\")\n",
    "    print(f\"new dataframe has shape {cleaned_df.shape}\")\n",
    "\n",
    "    return cleaned_df,to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84560ad9-9859-49e8-a33f-d368d43367b9",
   "metadata": {},
   "source": [
    "## IMPORT THE DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0798da49-d10d-47a2-88b2-2871f44b1f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing datasets\n",
    "filepaths_left={\n",
    "    \"pvs1_gps_mpu\" : r\"archive/PVS 1/dataset_gps_mpu_left.csv\",\n",
    "    \"pvs2_gps_mpu\" : r\"archive/PVS 2/dataset_gps_mpu_left.csv\",\n",
    "    \"pvs3_gps_mpu\" : r\"archive/PVS 3/dataset_gps_mpu_left.csv\",\n",
    "    \"pvs4_gps_mpu\" : r\"archive/PVS 4/dataset_gps_mpu_left.csv\",\n",
    "    \"pvs5_gps_mpu\" : r\"archive/PVS 5/dataset_gps_mpu_left.csv\",\n",
    "    \"pvs6_gps_mpu\" : r\"archive/PVS 6/dataset_gps_mpu_left.csv\",\n",
    "    \"pvs7_gps_mpu\" : r\"archive/PVS 7/dataset_gps_mpu_left.csv\",\n",
    "    \"pvs8_gps_mpu\" : r\"archive/PVS 8/dataset_gps_mpu_left.csv\",\n",
    "    \"pvs9_gps_mpu\" : r\"archive/PVS 9/dataset_gps_mpu_left.csv\"\n",
    "}\n",
    "\n",
    "filepaths_right={\n",
    "    \"pvs1_gps_mpu\" : r\"archive/PVS 1/dataset_gps_mpu_right.csv\",\n",
    "    \"pvs2_gps_mpu\" : r\"archive/PVS 2/dataset_gps_mpu_right.csv\",\n",
    "    \"pvs3_gps_mpu\" : r\"archive/PVS 3/dataset_gps_mpu_right.csv\",\n",
    "    \"pvs4_gps_mpu\" : r\"archive/PVS 4/dataset_gps_mpu_right.csv\",\n",
    "    \"pvs5_gps_mpu\" : r\"archive/PVS 5/dataset_gps_mpu_right.csv\",\n",
    "    \"pvs6_gps_mpu\" : r\"archive/PVS 6/dataset_gps_mpu_right.csv\",\n",
    "    \"pvs7_gps_mpu\" : r\"archive/PVS 7/dataset_gps_mpu_right.csv\",\n",
    "    \"pvs8_gps_mpu\" : r\"archive/PVS 8/dataset_gps_mpu_right.csv\",\n",
    "    \"pvs9_gps_mpu\" : r\"archive/PVS 9/dataset_gps_mpu_right.csv\"\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "# Left and right corresponds to sensordata from either side of the cars\n",
    "datasets_left = {name:pd.read_csv(path) for name,path in filepaths_left.items()}\n",
    "datasets_right = {name:pd.read_csv(path) for name,path in filepaths_right.items()}\n",
    "\n",
    "datasets = {name: merge(datasets_left[name],datasets_right[name], name) for name in datasets_right.keys()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4af0a75",
   "metadata": {},
   "source": [
    "# PREPROCESS THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbaac874",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_rm_zeros = remove_zero_values(datasets)\n",
    "equalized_datasets = equalize_dfs(datasets_rm_zeros)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aa76c5-a134-48f0-9ae8-7945c324ebaf",
   "metadata": {},
   "source": [
    "## REDUCE THE DATA \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5384f283-a71a-4b09-875b-c0311064730e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of speed list for pvs1_gps_mpu :  96500\n",
      "Length of speed list for pvs2_gps_mpu :  96500\n",
      "Length of speed list for pvs3_gps_mpu :  96500\n",
      "Length of speed list for pvs4_gps_mpu :  91800\n",
      "Length of speed list for pvs5_gps_mpu :  91800\n",
      "Length of speed list for pvs6_gps_mpu :  91800\n",
      "Length of speed list for pvs7_gps_mpu :  87600\n",
      "Length of speed list for pvs8_gps_mpu :  87600\n",
      "Length of speed list for pvs9_gps_mpu :  87600\n",
      "Reducing pvs1_gps_mpu\n",
      "Reducing pvs2_gps_mpu\n",
      "Reducing pvs3_gps_mpu\n",
      "Reducing pvs4_gps_mpu\n",
      "Reducing pvs5_gps_mpu\n",
      "Reducing pvs6_gps_mpu\n",
      "Reducing pvs7_gps_mpu\n",
      "Reducing pvs8_gps_mpu\n",
      "Reducing pvs9_gps_mpu\n"
     ]
    }
   ],
   "source": [
    "#REDUCE THE DATA\n",
    "ratio=100\n",
    "rounded_datasets = round_dfs(equalized_datasets, ratio)\n",
    "\n",
    "for key, index in rounded_datasets.items():\n",
    "    print(\"Length of speed list for\", key, \": \", len(rounded_datasets[key]['speed']))\n",
    "\n",
    "datasets_reduced={name:reduce_df(rounded_datasets[name],ratio,name) for name in rounded_datasets.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4dd08b-85af-4f0e-b9c3-2c7895cb474f",
   "metadata": {},
   "source": [
    "## WORK ON CORRELATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e019ef1a-25a0-451c-9dbe-83cfb708cdd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highly correlated pair: speed_max-speed_mean,(correlation: 0.92)\n",
      "highly correlated pair: acc_x_dashboard_l_min-acc_x_dashboard_l_STD_,(correlation: 0.80)\n",
      "highly correlated pair: gyro_z_dashboard_l_min-gyro_z_dashboard_l_STD_,(correlation: 0.86)\n",
      "new dataframe has shape (4924, 33)\n"
     ]
    }
   ],
   "source": [
    "#define the window and concatenate the entire dataset\n",
    "test_track=1\n",
    "dfs_windowed={name:computeWindow(name,datasets_reduced[name],100) for name in datasets_reduced.keys()}\n",
    "X_train,y_train,X_test,y_test=train_test_my_split(dfs_windowed,test_track,numpy_conversion=False)\n",
    "corr_matrix_test1=compute_correlation(X_train,plot=False)\n",
    "new_not_correlated_test1=remove_highly_correlated(X_train,corr_matrix_test1,threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e31daffa-c9f2-449c-b992-c7e05c3c9dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highly correlated pair: speed_max-speed_mean,(correlation: 0.93)\n",
      "highly correlated pair: speed_min-speed_mean,(correlation: 0.83)\n",
      "highly correlated pair: acc_x_dashboard_l_mean-acc_z_dashboard_l_mean,(correlation: 0.81)\n",
      "highly correlated pair: speed_mean-gyro_x_dashboard_l_STD_,(correlation: 0.83)\n",
      "new dataframe has shape (4924, 33)\n"
     ]
    }
   ],
   "source": [
    "test_track=2\n",
    "dfs_windowed={name:computeWindow(name,datasets_reduced[name],100) for name in datasets_reduced.keys()}\n",
    "X_train,y_train,X_test,y_test=train_test_my_split(dfs_windowed,test_track,numpy_conversion=False)\n",
    "corr_matrix_test2=compute_correlation(X_train,plot=False)\n",
    "new_not_correlated_test2=remove_highly_correlated(X_train,corr_matrix_test2,threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "71f6ee67-21be-40f6-82bd-f8eae9c7ae0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highly correlated pair: speed_max-speed_mean,(correlation: 0.92)\n",
      "highly correlated pair: gyro_x_dashboard_l_max-gyro_x_dashboard_l_STD_,(correlation: 0.84)\n",
      "highly correlated pair: speed_min-speed_mean,(correlation: 0.82)\n",
      "highly correlated pair: acc_x_dashboard_l_mean-acc_z_dashboard_l_mean,(correlation: 0.80)\n",
      "new dataframe has shape (4924, 33)\n"
     ]
    }
   ],
   "source": [
    "test_track=3\n",
    "dfs_windowed={name:computeWindow(name,datasets_reduced[name],100) for name in datasets_reduced.keys()}\n",
    "X_train,y_train,X_test,y_test=train_test_my_split(dfs_windowed,test_track,numpy_conversion=False)\n",
    "corr_matrix_test3=compute_correlation(X_train,plot=False)\n",
    "new_not_correlated_test3=remove_highly_correlated(X_train,corr_matrix_test3,threshold=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cbdd06e8-f8e7-4f06-8380-df11c1cefc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new dataframe has shape (551800, 8)\n"
     ]
    }
   ],
   "source": [
    "#let's now try to compute correlation before windowing ( can be really helpful if we decide that it makes sense)\n",
    "test_track=3\n",
    "X_train,y_train,X_test,y_test=train_test_my_split(rounded_datasets,test_track,numpy_conversion=False)\n",
    "corr_matrix=compute_correlation(X_train,plot=False)\n",
    "new_not_correlated=remove_highly_correlated(X_train,corr_matrix,threshold=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b7e239-166d-402d-b809-ad22a49c6e56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
