{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ec03935-e373-428f-a845-66a34c0219ad",
   "metadata": {},
   "source": [
    "## TRAIN THE MODEL AND EVALUATE IT \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5ec4a13-9fbc-4f6e-9c9d-ea61b2bcbe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries for code\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.metrics import make_scorer, accuracy_score \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d65b83-5b4f-411f-9203-2c3ae02508f3",
   "metadata": {},
   "source": [
    "## DEFINE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a14fc20-06cd-4fdb-a8f9-ce3bed23426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define train_test splits, based on the track we define in test_track. \n",
    "#we use test_track for testing and the other two tracks for training\n",
    "#-->returns X_train,y_train,X_test,y_test as numpy arrays\n",
    "\n",
    "def train_test_my_split(dfs,test_track,numpy_conversion=True):\n",
    "    #take track three for testing and trcakk 1,2 for trainig\n",
    "    all_keys=list(dfs.keys())\n",
    "\n",
    "\n",
    "    if test_track==3:\n",
    "        train_indices=[0,1,3,4,6,7]\n",
    "        test_indices=[2,5,8]\n",
    "    elif test_track==2:\n",
    "        train_indices=[0,2,3,5,6,8]\n",
    "        test_indices=[1,4,7]\n",
    "    elif test_track==1:\n",
    "        train_indices=[1,2,4,5,7,8]\n",
    "        test_indices=[0,3,6]\n",
    "        \n",
    "\n",
    "    train_dfs = [dfs[all_keys[i]] for i in train_indices]\n",
    "    train_df = pd.concat(train_dfs, axis=0, ignore_index=True)\n",
    "\n",
    "    # Shuffle the training data\n",
    "    train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    test_dfs = [dfs[all_keys[i]] for i in test_indices]\n",
    "    test_df = pd.concat(test_dfs, axis=0, ignore_index=True)\n",
    "\n",
    "    \n",
    "    # Shuffle testing data\n",
    "    test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    #split features and targets\n",
    "    if numpy_conversion:\n",
    "        #split features and targets\n",
    "        X_train=train_df.iloc[:,:-1].to_numpy()\n",
    "        y_train=train_df.iloc[:,-1].to_numpy()\n",
    "    \n",
    "        X_test=test_df.iloc[:,:-1].to_numpy()\n",
    "        y_test=test_df.iloc[:,-1].to_numpy()\n",
    "    else:\n",
    "        #split features and targets\n",
    "        X_train=train_df.iloc[:,:-1]\n",
    "        y_train=train_df.iloc[:,-1]\n",
    "    \n",
    "        X_test=test_df.iloc[:,:-1]\n",
    "        y_test=test_df.iloc[:,-1]\n",
    "\n",
    "    return X_train,y_train,X_test,y_test\n",
    "\n",
    "#function for creating the model based on the parameter type\n",
    "#--> returns the model\n",
    "def create_model(type):\n",
    "    if type==\"RandomForest\":\n",
    "        return RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    elif type == \"SVM\":\n",
    "        return SVC(kernel=\"rbf\", C=1.0)\n",
    "    elif type == \"lr\":\n",
    "        return LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "#test the model on the TEST set, take as input the NON-WINDOWED datasets\n",
    "#-->returns the accuracy on the test set\n",
    "\n",
    "def test_model(X_train,y_train,X_test,y_test,model_type,test_track=3):\n",
    "    #create the  model \n",
    "\n",
    "    #print(\"Test the model\")\n",
    "    #print(X_train.shape)\n",
    "    #print(X_test.shape)\n",
    "\n",
    "    \n",
    "\n",
    "    model=create_model(model_type)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Test set evaluation\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"Test set accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    return test_accuracy\n",
    "    \n",
    "\n",
    "# function for cross-fold evaluation, with num_folds folds, taken as a parameter\n",
    "#--> returns average accuracy for the specific hyperparameters configuration defined as input\n",
    "\n",
    "def evaluate_model(X_train,y_train,model_type,num_folds,test_track=3):\n",
    "    \n",
    "    #create the  model \n",
    "    model=create_model(model_type)\n",
    "\n",
    "    #APPLY CROSS-FOLDER EVALUATION\n",
    "\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_accuracies = []\n",
    "    \n",
    "    for i,(train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "        X_ttrain, X_val = X_train[train_index], X_train[val_index] \n",
    "        y_ttrain, y_val = y_train[train_index], y_train[val_index]\n",
    "            \n",
    "        model.fit(X_ttrain, y_ttrain) \n",
    "        y_pred = model.predict(X_val) \n",
    "        \n",
    "        accuracy = accuracy_score(y_val, y_pred) \n",
    "        #print((y_val != y_pred).sum())\n",
    "        print(f'fold {i} accuracy:', accuracy)\n",
    "        fold_accuracies.append(accuracy)\n",
    "\n",
    "    average_accuracy = sum(fold_accuracies) / num_folds\n",
    "    print('average of folds',average_accuracy)\n",
    "\n",
    "    return average_accuracy\n",
    "\n",
    "#intermediate function, used for: \n",
    "#windowing based on the window size\n",
    "#-->returns X_train,y_train,X_test,y_test based on track defined in test_track\n",
    "\n",
    "\n",
    "def window_and_split(dfs,window_size,test_track=3,numpy_conversion=True):\n",
    "    # Load the windowed data\n",
    "    with open(f\"dfs_windowed_{window_size}.pkl\", \"rb\") as file:\n",
    "        dfs_windowed = pickle.load(file)\n",
    "\n",
    "    return train_test_my_split(dfs_windowed,test_track,numpy_conversion)\n",
    "\n",
    "#receives X_train and X_test ALREADY SCALED  and returns pca datasets, as numpy arrays.\n",
    "def apply_PCA(X_train,X_test,threshold):\n",
    "\n",
    "    X_train_scaled = pd.DataFrame(X_train)\n",
    "    X_test_scaled = pd.DataFrame(X_test)\n",
    "\n",
    "    pca=PCA()\n",
    "\n",
    "    pca=PCA(n_components=threshold, random_state=29)\n",
    "    X_train_pca=pca.fit_transform(X_train_scaled)\n",
    "    X_test_pca=pca.transform(X_test_scaled)\n",
    "\n",
    "    return X_train_pca,X_test_pca\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "#main function, takes hyperparameters options, model type and num_folds for k-fold\n",
    "#tries all configurations on the evaluation set\n",
    "#test the best configuration on the trainig set \n",
    "\n",
    "def tuning_and_evaluation(dfs,window_sizes,model_type,num_folds,test_track=3,threshold_pca=1):\n",
    "\n",
    "    #method for deciding if we wanna convert or not into numpy, not important\n",
    "    if threshold_pca!=1:\n",
    "        numpy_conversion=True\n",
    "    else:\n",
    "        numpy_conversion=True\n",
    "\n",
    "    \n",
    "    #initialize optimal results\n",
    "    best_accuracy=0\n",
    "    best_window_size=0\n",
    "\n",
    "\n",
    "    #store all the average accuracies with different hyperparameters inside an arrray\n",
    "    tot_accuracies=[]\n",
    "\n",
    "    for window_size in window_sizes:\n",
    "        print(f\"--------------------\")\n",
    "        print(f\"EVALUATE window_size: {window_size}\")\n",
    "        \n",
    "        X_train,y_train,X_test,y_test=window_and_split(datasets_reduced,window_size,test_track,numpy_conversion)\n",
    "        \n",
    "        #scale values\n",
    "        scaler=StandardScaler()\n",
    "        X_train_scaled=scaler.fit_transform(X_train)\n",
    "        X_test_scaled=scaler.transform(X_test)\n",
    "    \n",
    "\n",
    "\n",
    "        if threshold_pca!=1:\n",
    "            print(\"apply pca for evaluation\")\n",
    "            X_train_pca_ev,X_test_pca_ev=apply_PCA(X_train_scaled,X_test_scaled,threshold_pca)\n",
    "            print(f\"dataset has {X_train_pca_ev.shape} \")\n",
    "\n",
    "        \n",
    "        \n",
    "        accuracy=evaluate_model(X_train_pca_ev,y_train,model_type,num_folds,test_track)\n",
    "        tot_accuracies.append(accuracy)\n",
    "        \n",
    "        \n",
    "        #update optimal results if needed\n",
    "        if accuracy>best_accuracy:\n",
    "            best_accuracy=accuracy\n",
    "            best_window_size=window_size\n",
    "            \n",
    "            \n",
    "    print(f\"Best window size: {best_window_size} with accuracy: {best_accuracy}\")\n",
    "    print(\"test best model on TEST data\")\n",
    "\n",
    "    \n",
    "    X_train,y_train,X_test,y_test=window_and_split(datasets_reduced,best_window_size,test_track,numpy_conversion)\n",
    "\n",
    "    #scale values\n",
    "    scaler=StandardScaler()\n",
    "    X_train_scaled=scaler.fit_transform(X_train)\n",
    "    X_test_scaled=scaler.transform(X_test)\n",
    "\n",
    "    if threshold_pca!=1:\n",
    "            print(\"apply pca for test\")\n",
    "            X_train_pca_test,X_test_pca_test=apply_PCA(X_train_scaled,X_test_scaled,threshold_pca)\n",
    "\n",
    "\n",
    "    \n",
    "    return test_model(X_train_pca_test,y_train,X_test_pca_test,y_test,model_type,test_track)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac53b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define train_test splits, based on the track we define in test_track. \n",
    "#we use test_track for testing and the other two tracks for training\n",
    "#-->returns X_train,y_train,X_test,y_test as numpy arrays\n",
    "\n",
    "def train_test_my_split(dfs,test_track,numpy_conversion=True):\n",
    "    #take track three for testing and trcakk 1,2 for trainig\n",
    "    all_keys=list(dfs.keys())\n",
    "\n",
    "\n",
    "    if test_track==3:\n",
    "        train_indices=[0,1,3,4,6,7]\n",
    "        test_indices=[2,5,8]\n",
    "    elif test_track==2:\n",
    "        train_indices=[0,2,3,5,6,8]\n",
    "        test_indices=[1,4,7]\n",
    "    elif test_track==1:\n",
    "        train_indices=[1,2,4,5,7,8]\n",
    "        test_indices=[0,3,6]\n",
    "        \n",
    "\n",
    "    train_dfs = [dfs[all_keys[i]] for i in train_indices]\n",
    "    train_df = pd.concat(train_dfs, axis=0, ignore_index=True)\n",
    "\n",
    "    # Shuffle the training data\n",
    "    train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    test_dfs = [dfs[all_keys[i]] for i in test_indices]\n",
    "    test_df = pd.concat(test_dfs, axis=0, ignore_index=True)\n",
    "\n",
    "    \n",
    "    # Shuffle testing data\n",
    "    test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    #split features and targets\n",
    "    if numpy_conversion:\n",
    "        #split features and targets\n",
    "        X_train=train_df.iloc[:,:-1].to_numpy()\n",
    "        y_train=train_df.iloc[:,-1].to_numpy()\n",
    "    \n",
    "        X_test=test_df.iloc[:,:-1].to_numpy()\n",
    "        y_test=test_df.iloc[:,-1].to_numpy()\n",
    "    else:\n",
    "        #split features and targets\n",
    "        X_train=train_df.iloc[:,:-1]\n",
    "        y_train=train_df.iloc[:,-1]\n",
    "    \n",
    "        X_test=test_df.iloc[:,:-1]\n",
    "        y_test=test_df.iloc[:,-1]\n",
    "\n",
    "    return X_train,y_train,X_test,y_test\n",
    "\n",
    "#function for creating the model based on the parameter type\n",
    "#--> returns the model\n",
    "def create_model(type):\n",
    "    if type==\"RandomForest\":\n",
    "        return RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    elif type == \"SVM\":\n",
    "        return SVC(kernel=\"rbf\", C=1.0)\n",
    "    elif type == \"lr\":\n",
    "        return LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "#test the model on the TEST set, take as input the NON-WINDOWED datasets\n",
    "#-->returns the accuracy on the test set\n",
    "\n",
    "def test_model(X_train,y_train,X_test,y_test,model_type,test_track=3):\n",
    "    #create the  model \n",
    "\n",
    "    #print(\"Test the model\")\n",
    "    #print(X_train.shape)\n",
    "    #print(X_test.shape)\n",
    "\n",
    "    \n",
    "\n",
    "    model=create_model(model_type)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Test set evaluation\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"Test set accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    return test_accuracy\n",
    "    \n",
    "\n",
    "# function for cross-fold evaluation, with num_folds folds, taken as a parameter\n",
    "#--> returns average accuracy for the specific hyperparameters configuration defined as input\n",
    "\n",
    "def evaluate_model(X_train,y_train,model_type,num_folds,test_track=3):\n",
    "    \n",
    "    #create the  model \n",
    "    model=create_model(model_type)\n",
    "\n",
    "    #APPLY CROSS-FOLDER EVALUATION\n",
    "\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_accuracies = []\n",
    "    \n",
    "    for i,(train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "        X_ttrain, X_val = X_train[train_index], X_train[val_index] \n",
    "        y_ttrain, y_val = y_train[train_index], y_train[val_index]\n",
    "            \n",
    "        model.fit(X_ttrain, y_ttrain) \n",
    "        y_pred = model.predict(X_val) \n",
    "        \n",
    "        accuracy = accuracy_score(y_val, y_pred) \n",
    "        #print((y_val != y_pred).sum())\n",
    "        print(f'fold {i} accuracy:', accuracy)\n",
    "        fold_accuracies.append(accuracy)\n",
    "\n",
    "    average_accuracy = sum(fold_accuracies) / num_folds\n",
    "    print('average of folds',average_accuracy)\n",
    "\n",
    "    return average_accuracy\n",
    "\n",
    "#intermediate function, used for: \n",
    "#windowing based on the window size\n",
    "#-->returns X_train,y_train,X_test,y_test based on track defined in test_track\n",
    "\n",
    "\n",
    "def window_and_split(dfs,window_size,test_track=3,numpy_conversion=True):\n",
    "    # Load the windowed data\n",
    "    with open(f\"dfs_windowed_{window_size}.pkl\", \"rb\") as file:\n",
    "        dfs_windowed = pickle.load(file)\n",
    "\n",
    "    return train_test_my_split(dfs_windowed,test_track,numpy_conversion)\n",
    "\n",
    "#receives X_train and X_test ALREADY SCALED  and returns pca datasets, as numpy arrays.\n",
    "def apply_PCA(X_train,X_test,threshold):\n",
    "\n",
    "    X_train_scaled = pd.DataFrame(X_train)\n",
    "    X_test_scaled = pd.DataFrame(X_test)\n",
    "\n",
    "    pca=PCA()\n",
    "\n",
    "    pca=PCA(n_components=threshold, random_state=29)\n",
    "    X_train_pca=pca.fit_transform(X_train_scaled)\n",
    "    X_test_pca=pca.transform(X_test_scaled)\n",
    "\n",
    "    return X_train_pca,X_test_pca\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "#main function, takes hyperparameters options, model type and num_folds for k-fold\n",
    "#tries all configurations on the evaluation set\n",
    "#test the best configuration on the trainig set \n",
    "\n",
    "def tuning_and_evaluation(dfs,window_sizes,model_type,num_folds,test_track=3,threshold_pca=1):\n",
    "\n",
    "    #method for deciding if we wanna convert or not into numpy, not important\n",
    "    if threshold_pca!=1:\n",
    "        numpy_conversion=True\n",
    "    else:\n",
    "        numpy_conversion=True\n",
    "\n",
    "    \n",
    "    #initialize optimal results\n",
    "    best_accuracy=0\n",
    "    best_window_size=0\n",
    "\n",
    "\n",
    "    #store all the average accuracies with different hyperparameters inside an arrray\n",
    "    tot_accuracies=[]\n",
    "\n",
    "    for window_size in window_sizes:\n",
    "        print(f\"--------------------\")\n",
    "        print(f\"EVALUATE window_size: {window_size}\")\n",
    "        \n",
    "        X_train,y_train,X_test,y_test=window_and_split(datasets_reduced,window_size,test_track,numpy_conversion)\n",
    "        \n",
    "        #scale values\n",
    "        scaler=StandardScaler()\n",
    "        X_train_scaled=scaler.fit_transform(X_train)\n",
    "        X_test_scaled=scaler.transform(X_test)\n",
    "    \n",
    "\n",
    "\n",
    "        if threshold_pca!=1:\n",
    "            print(\"apply pca for evaluation\")\n",
    "            X_train_pca_ev,X_test_pca_ev=apply_PCA(X_train_scaled,X_test_scaled,threshold_pca)\n",
    "            print(f\"dataset has {X_train_pca_ev.shape} \")\n",
    "\n",
    "        \n",
    "        \n",
    "        accuracy=evaluate_model(X_train_pca_ev,y_train,model_type,num_folds,test_track)\n",
    "        tot_accuracies.append(accuracy)\n",
    "        \n",
    "        \n",
    "        #update optimal results if needed\n",
    "        if accuracy>best_accuracy:\n",
    "            best_accuracy=accuracy\n",
    "            best_window_size=window_size\n",
    "            \n",
    "            \n",
    "    print(f\"Best window size: {best_window_size} with accuracy: {best_accuracy}\")\n",
    "    print(\"test best model on TEST data\")\n",
    "\n",
    "    \n",
    "    X_train,y_train,X_test,y_test=window_and_split(datasets_reduced,best_window_size,test_track,numpy_conversion)\n",
    "\n",
    "    #scale values\n",
    "    scaler=StandardScaler()\n",
    "    X_train_scaled=scaler.fit_transform(X_train)\n",
    "    X_test_scaled=scaler.transform(X_test)\n",
    "\n",
    "    if threshold_pca!=1:\n",
    "            print(\"apply pca for test\")\n",
    "            X_train_pca_test,X_test_pca_test=apply_PCA(X_train_scaled,X_test_scaled,threshold_pca)\n",
    "\n",
    "\n",
    "    \n",
    "    return test_model(X_train_pca_test,y_train,X_test_pca_test,y_test,model_type,test_track)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84560ad9-9859-49e8-a33f-d368d43367b9",
   "metadata": {},
   "source": [
    "## IMPORT THE DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4a2a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets_reduced, which is the temporal data truncated\n",
    "with open(\"datasets_reduced.pkl\", \"rb\") as file:\n",
    "    datasets_reduced = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a558e1-b198-415b-ad0b-15e509d9b77b",
   "metadata": {},
   "source": [
    "## TEST THE MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ada19aa6-4362-4986-8227-f6e3f36b6d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "PCA threshold: 0.6\n",
      "--------------------\n",
      "EVALUATE window_size: 50\n",
      "apply pca for evaluation\n",
      "dataset has (359100, 6) \n",
      "fold 0 accuracy: 0.8293233082706767\n",
      "fold 1 accuracy: 0.8311473127262601\n",
      "fold 2 accuracy: 0.8304511278195489\n",
      "fold 3 accuracy: 0.8296157059314954\n",
      "fold 4 accuracy: 0.8299359509885825\n",
      "average of folds 0.8300946811473127\n",
      "--------------------\n",
      "EVALUATE window_size: 100\n",
      "apply pca for evaluation\n",
      "dataset has (358800, 5) \n",
      "fold 0 accuracy: 0.9025083612040133\n",
      "fold 1 accuracy: 0.8997491638795987\n",
      "fold 2 accuracy: 0.9\n",
      "fold 3 accuracy: 0.9009197324414716\n",
      "fold 4 accuracy: 0.9023411371237459\n",
      "average of folds 0.901103678929766\n",
      "--------------------\n",
      "EVALUATE window_size: 200\n",
      "apply pca for evaluation\n",
      "dataset has (358200, 5) \n",
      "fold 0 accuracy: 0.9449609156895589\n",
      "fold 1 accuracy: 0.9441792294807371\n",
      "fold 2 accuracy: 0.9431323283082077\n",
      "fold 3 accuracy: 0.9439838079285315\n",
      "fold 4 accuracy: 0.941247906197655\n",
      "average of folds 0.9435008375209379\n",
      "--------------------\n",
      "EVALUATE window_size: 300\n",
      "apply pca for evaluation\n",
      "dataset has (357600, 5) \n",
      "fold 0 accuracy: 0.9736157718120806\n",
      "fold 1 accuracy: 0.9725251677852349\n",
      "fold 2 accuracy: 0.9735318791946309\n",
      "fold 3 accuracy: 0.9751817673378076\n",
      "fold 4 accuracy: 0.9737276286353468\n",
      "average of folds 0.9737164429530202\n",
      "--------------------\n",
      "EVALUATE window_size: 400\n",
      "apply pca for evaluation\n",
      "dataset has (357000, 5) \n",
      "fold 0 accuracy: 0.9824509803921568\n",
      "fold 1 accuracy: 0.9811624649859944\n",
      "fold 2 accuracy: 0.9813165266106443\n",
      "fold 3 accuracy: 0.9824929971988795\n",
      "fold 4 accuracy: 0.9827170868347339\n",
      "average of folds 0.9820280112044818\n",
      "--------------------\n",
      "EVALUATE window_size: 500\n",
      "apply pca for evaluation\n",
      "dataset has (356400, 5) \n",
      "fold 0 accuracy: 0.9867424242424242\n",
      "fold 1 accuracy: 0.9864758698092031\n",
      "fold 2 accuracy: 0.9873316498316498\n",
      "fold 3 accuracy: 0.9856200897867564\n",
      "fold 4 accuracy: 0.985662177328844\n",
      "average of folds 0.9863664421997754\n",
      "Best window size: 500 with accuracy: 0.9863664421997754\n",
      "test best model on TEST data\n",
      "apply pca for test\n",
      "Test set accuracy: 0.6150\n",
      "Time taken for PCA threshold 0.6: 7922.16 seconds\n",
      "----------------------\n",
      "PCA threshold: 0.7\n",
      "--------------------\n",
      "EVALUATE window_size: 50\n",
      "apply pca for evaluation\n",
      "dataset has (359100, 8) \n",
      "fold 0 accuracy: 0.8346700083542189\n",
      "fold 1 accuracy: 0.8364243943191312\n",
      "fold 2 accuracy: 0.8373851294903927\n",
      "fold 3 accuracy: 0.8346282372598162\n",
      "fold 4 accuracy: 0.8365079365079365\n",
      "average of folds 0.8359231411862991\n",
      "--------------------\n",
      "EVALUATE window_size: 100\n",
      "apply pca for evaluation\n",
      "dataset has (358800, 8) \n",
      "fold 0 accuracy: 0.9178790412486064\n",
      "fold 1 accuracy: 0.9182134894091416\n",
      "fold 2 accuracy: 0.9181159420289855\n",
      "fold 3 accuracy: 0.9191053511705686\n",
      "fold 4 accuracy: 0.9169453734671126\n",
      "average of folds 0.918051839464883\n",
      "--------------------\n",
      "EVALUATE window_size: 200\n",
      "apply pca for evaluation\n",
      "dataset has (358200, 8) \n",
      "fold 0 accuracy: 0.9568257956448911\n",
      "fold 1 accuracy: 0.9555834729201563\n",
      "fold 2 accuracy: 0.9547040759352318\n",
      "fold 3 accuracy: 0.9556951423785595\n",
      "fold 4 accuracy: 0.9541178112786153\n",
      "average of folds 0.9553852596314908\n",
      "--------------------\n",
      "EVALUATE window_size: 300\n",
      "apply pca for evaluation\n",
      "dataset has (357600, 8) \n",
      "fold 0 accuracy: 0.9777964205816555\n",
      "fold 1 accuracy: 0.9772511185682327\n",
      "fold 2 accuracy: 0.9775866890380314\n",
      "fold 3 accuracy: 0.9778942953020134\n",
      "fold 4 accuracy: 0.97751677852349\n",
      "average of folds 0.9776090604026846\n",
      "--------------------\n",
      "EVALUATE window_size: 400\n",
      "apply pca for evaluation\n",
      "dataset has (357000, 8) \n",
      "fold 0 accuracy: 0.9843697478991597\n",
      "fold 1 accuracy: 0.9838375350140056\n",
      "fold 2 accuracy: 0.9834453781512605\n",
      "fold 3 accuracy: 0.984859943977591\n",
      "fold 4 accuracy: 0.984313725490196\n",
      "average of folds 0.9841652661064426\n",
      "--------------------\n",
      "EVALUATE window_size: 500\n",
      "apply pca for evaluation\n",
      "dataset has (356400, 8) \n",
      "fold 0 accuracy: 0.988692480359147\n",
      "fold 1 accuracy: 0.9879910213243547\n",
      "fold 2 accuracy: 0.9884399551066217\n",
      "fold 3 accuracy: 0.9876543209876543\n",
      "fold 4 accuracy: 0.9879629629629629\n",
      "average of folds 0.9881481481481481\n",
      "Best window size: 500 with accuracy: 0.9881481481481481\n",
      "test best model on TEST data\n",
      "apply pca for test\n",
      "Test set accuracy: 0.6782\n",
      "Time taken for PCA threshold 0.7: 7347.38 seconds\n",
      "----------------------\n",
      "PCA threshold: 0.8\n",
      "--------------------\n",
      "EVALUATE window_size: 50\n",
      "apply pca for evaluation\n",
      "dataset has (359100, 12) \n",
      "fold 0 accuracy: 0.8634781397939293\n",
      "fold 1 accuracy: 0.8637844611528822\n",
      "fold 2 accuracy: 0.8669869117237539\n",
      "fold 3 accuracy: 0.866123642439432\n",
      "fold 4 accuracy: 0.8659983291562239\n",
      "average of folds 0.8652742968532442\n",
      "--------------------\n",
      "EVALUATE window_size: 100\n",
      "apply pca for evaluation\n",
      "dataset has (358800, 12) \n",
      "fold 0 accuracy: 0.9390189520624304\n",
      "fold 1 accuracy: 0.938628762541806\n",
      "fold 2 accuracy: 0.9415830546265329\n",
      "fold 3 accuracy: 0.9401616499442587\n",
      "fold 4 accuracy: 0.9393394648829432\n",
      "average of folds 0.9397463768115941\n",
      "--------------------\n",
      "EVALUATE window_size: 200\n",
      "apply pca for evaluation\n",
      "dataset has (358200, 11) \n",
      "fold 0 accuracy: 0.9589056393076494\n",
      "fold 1 accuracy: 0.9590173087660525\n",
      "fold 2 accuracy: 0.956853713009492\n",
      "fold 3 accuracy: 0.9583333333333334\n",
      "fold 4 accuracy: 0.9581099944165271\n",
      "average of folds 0.9582439977666108\n",
      "--------------------\n",
      "EVALUATE window_size: 300\n",
      "apply pca for evaluation\n",
      "dataset has (357600, 11) \n",
      "fold 0 accuracy: 0.9816414988814318\n",
      "fold 1 accuracy: 0.9827600671140939\n",
      "fold 2 accuracy: 0.9828159955257271\n",
      "fold 3 accuracy: 0.9826901565995526\n",
      "fold 4 accuracy: 0.9824105145413871\n",
      "average of folds 0.9824636465324386\n",
      "--------------------\n",
      "EVALUATE window_size: 400\n",
      "apply pca for evaluation\n",
      "dataset has (357000, 11) \n",
      "fold 0 accuracy: 0.9865686274509804\n",
      "fold 1 accuracy: 0.9857703081232493\n",
      "fold 2 accuracy: 0.9865826330532212\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA threshold: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthreshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m final_results\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtuning_and_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets_reduced\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwindow_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_folds\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_track\u001b[49m\u001b[43m,\u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     18\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     19\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[1;32mIn[8], line 189\u001b[0m, in \u001b[0;36mtuning_and_evaluation\u001b[1;34m(dfs, window_sizes, model_type, num_folds, test_track, threshold_pca)\u001b[0m\n\u001b[0;32m    184\u001b[0m     X_train_pca_ev,X_test_pca_ev\u001b[38;5;241m=\u001b[39mapply_PCA(X_train_scaled,X_test_scaled,threshold_pca)\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train_pca_ev\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 189\u001b[0m accuracy\u001b[38;5;241m=\u001b[39m\u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_pca_ev\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_folds\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_track\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m tot_accuracies\u001b[38;5;241m.\u001b[39mappend(accuracy)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m#update optimal results if needed\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 106\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(X_train, y_train, model_type, num_folds, test_track)\u001b[0m\n\u001b[0;32m    103\u001b[0m X_ttrain, X_val \u001b[38;5;241m=\u001b[39m X_train[train_index], X_train[val_index] \n\u001b[0;32m    104\u001b[0m y_ttrain, y_val \u001b[38;5;241m=\u001b[39m y_train[train_index], y_train[val_index]\n\u001b[1;32m--> 106\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_ttrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_ttrain\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m    107\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val) \n\u001b[0;32m    109\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_val, y_pred) \n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\tree\\_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    930\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \n\u001b[0;32m    932\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import io\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import time\n",
    "\n",
    "model_type=\"RandomForest\"\n",
    "window_sizes500]\n",
    "num_folds = 5\n",
    "test_track=3\n",
    "thresholds_pca=[0.6,0.7,0.8,0.9,0.95]\n",
    "final_results=[]\n",
    "\n",
    "for threshold in thresholds_pca:\n",
    "    start_time = time.time()\n",
    "    print(f\"----------------------\")\n",
    "    print(f\"PCA threshold: {threshold}\")\n",
    "    final_results.append(tuning_and_evaluation(datasets_reduced,window_sizes,model_type,num_folds,test_track,threshold))\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Time taken for PCA threshold {threshold}: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d4a795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
