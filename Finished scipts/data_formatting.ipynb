{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproccesing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.metrics import make_scorer, accuracy_score \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a14fc20-06cd-4fdb-a8f9-ce3bed23426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for merging left and right dataframe\n",
    "def merge(left, right, name ):\n",
    "    pvs= pd.concat([left, right], axis=1)\n",
    "    length= len(left)\n",
    "\n",
    "    # Renaming labels\n",
    "    pvs.columns = [\n",
    "    'timestamp', 'acc_x_dashboard_l', 'acc_y_dashboard_l', 'acc_z_dashboard_l',\n",
    "    'acc_x_above_suspension_l', 'acc_y_above_suspension_l', 'acc_z_above_suspension_l', \n",
    "    'acc_x_below_suspension_l', 'acc_y_below_suspension_l', 'acc_z_below_suspension_l', \n",
    "    'gyro_x_dashboard_l', 'gyro_y_dashboard_l', 'gyro_z_dashboard_l', 'gyro_x_above_suspension_l', \n",
    "    'gyro_y_above_suspension_l', 'gyro_z_above_suspension_l', 'gyro_x_below_suspension_l', \n",
    "    'gyro_y_below_suspension_l', 'gyro_z_below_suspension_l', 'mag_x_dashboard_l', 'mag_y_dashboard_l', \n",
    "    'mag_z_dashboard_l', 'mag_x_above_suspension_l', 'mag_y_above_suspension_l', 'mag_z_above_suspension_l', \n",
    "    'temp_dashboard_l', 'temp_above_suspension_l', 'temp_below_suspension_l', 'timestamp_gps', \n",
    "    'latitude', 'longitude', 'speed', 'timestamp', 'acc_x_dashboard_r', 'acc_y_dashboard_r', \n",
    "    'acc_z_dashboard_r', 'acc_x_above_suspension_r', 'acc_y_above_suspension_r', \n",
    "    'acc_z_above_suspension_r', 'acc_x_below_suspension_r', 'acc_y_below_suspension_r', \n",
    "    'acc_z_below_suspension_r', 'gyro_x_dashboard_r', 'gyro_y_dashboard_r', 'gyro_z_dashboard_r', \n",
    "    'gyro_x_above_suspension_r', 'gyro_y_above_suspension_r', 'gyro_z_above_suspension_r', \n",
    "    'gyro_x_below_suspension_r', 'gyro_y_below_suspension_r', 'gyro_z_below_suspension_r', \n",
    "    'mag_x_dashboard_r', 'mag_y_dashboard_r', 'mag_z_dashboard_r', 'mag_x_above_suspension_r', \n",
    "    'mag_y_above_suspension_r', 'mag_z_above_suspension_r', 'temp_dashboard_r', 'temp_above_suspension_r', \n",
    "    'temp_below_suspension_r', 'timestamp_gps', 'latitude', 'longitude', 'speed'\n",
    "    ]\n",
    "    \n",
    "\n",
    "    # Remove duplicate columns (those with the same name, e.g., 'timestamp_gps', 'latitude', etc.)\n",
    "    pvs_removed = pvs.loc[:, ~pvs.columns.duplicated()]\n",
    "\n",
    "    # Columns to keep. We try these first\n",
    "    to_keep=[\"timestamp\",\n",
    "             \"acc_x_dashboard_l\",\n",
    "             \"acc_y_dashboard_l\",\n",
    "             \"acc_z_dashboard_l\",\n",
    "             \"speed\",\n",
    "             \"gyro_x_dashboard_l\",\n",
    "             \"gyro_y_dashboard_l\",\n",
    "             \"gyro_z_dashboard_l\"\n",
    "            ]\n",
    "    \n",
    "    pvs_removed=pvs_removed[to_keep]\n",
    "\n",
    "    #create the driver column ( target )\n",
    "    if name in (\"pvs1_gps_mpu\",\"pvs2_gps_mpu\",\"pvs3_gps_mpu\"):\n",
    "        pvs_removed['Driver'] = 1\n",
    "    elif name in (\"pvs4_gps_mpu\",\"pvs5_gps_mpu\",\"pvs6_gps_mpu\"):\n",
    "        pvs_removed['Driver'] = 2\n",
    "    else: \n",
    "        pvs_removed['Driver'] = 3\n",
    "        \n",
    "    return pvs_removed\n",
    "\n",
    "#Drivers do not drive in the start and end of each route. Function for removing these indicies\n",
    "def remove_zero_values(dfs):\n",
    "\n",
    "    new = dfs\n",
    "\n",
    "    non_zero_indices = {}\n",
    "    threshold = 0.1  # Define the threshold to detect non-zero speeds\n",
    "\n",
    "    for key, df in new.items():\n",
    "\n",
    "        first_non_zero_index = (df['speed'].abs() > threshold).idxmax()\n",
    "        last_non_zero_index = (df['speed'][::-1].abs() > threshold).idxmax()\n",
    "\n",
    "        non_zero_indices[key] = (first_non_zero_index, last_non_zero_index)\n",
    "\n",
    "    #print(non_zero_indices)\n",
    "\n",
    "    # Iterate through each dataset and its respective non-zero index\n",
    "    for key, index in non_zero_indices.items():\n",
    "        new[key] = new[key].iloc[index[0]:index[1]].reset_index(drop=True)  # Remove rows up to the index and reset index\n",
    "\n",
    "    return new\n",
    "\n",
    "#Function for making dfs equal size in order to avoid bias. Each route is truncated to the same length\n",
    "def equalize_dfs(dfs):\n",
    "\n",
    "    new = dfs\n",
    "\n",
    "    # Split DataFrames into three routes\n",
    "    dfs_keys = list(new.keys())  # Get all keys in the dictionary\n",
    "    first_route_keys = dfs_keys[:3]  # First three keys for the first route\n",
    "    second_route_keys = dfs_keys[3:6]  # Next three keys for the second route\n",
    "    third_route_keys = dfs_keys[6:]  # Last three keys for the third route\n",
    "\n",
    "    # Calculate the minimum lengths for each route\n",
    "    min_length_first_route = min(len(new[key]) for key in first_route_keys)\n",
    "    min_length_second_route = min(len(new[key]) for key in second_route_keys)\n",
    "    min_length_third_route = min(len(new[key]) for key in third_route_keys)\n",
    "\n",
    "    #print(f\"Shortest length in the first route: {min_length_first_route}\")\n",
    "    #print(f\"Shortest length in the second route: {min_length_second_route}\")\n",
    "    #print(f\"Shortest length in the third route: {min_length_third_route}\")\n",
    "\n",
    "    # Truncate all DataFrames in each route to the respective minimum length\n",
    "    for key in first_route_keys:\n",
    "        new[key] = new[key].iloc[:min_length_first_route].reset_index(drop=True)\n",
    "\n",
    "    for key in second_route_keys:\n",
    "        new[key] = new[key].iloc[:min_length_second_route].reset_index(drop=True)\n",
    "\n",
    "    for key in third_route_keys:\n",
    "        new[key] = new[key].iloc[:min_length_third_route].reset_index(drop=True)\n",
    "\n",
    "    return new\n",
    "\n",
    "#Need to make the ratio between dfs and ratio to be an integer in order for dataframe reduction to work\n",
    "def round_dfs(dfs, ratio):\n",
    "\n",
    "    new = dfs\n",
    "\n",
    "    for key, df in new.items():\n",
    "\n",
    "        rounded_length = len(df) - (len(df) % ratio)\n",
    "\n",
    "        new[key] = df.iloc[:rounded_length].reset_index(drop=True)\n",
    "\n",
    "    return new\n",
    "\n",
    "\n",
    "def reduce_df(df, ratio, name):\n",
    "    new = pd.DataFrame()\n",
    "    print(f\"Reducing {name}\")\n",
    "    \n",
    "    # Iterate in steps of 'ratio' through the DataFrame\n",
    "    for i in range(0, len(df) - ratio + 1, ratio):\n",
    "        # Select the rows from i to i+ratio (inclusive of i, exclusive of i+ratio)\n",
    "        subset = df.iloc[i:i+ratio]\n",
    "        \n",
    "        # Calculate the mean for each column in the selected rows\n",
    "        merged_row = subset.mean(axis=0)\n",
    "        \n",
    "        # Append the result as a new row to the DataFrame `new`\n",
    "        new = pd.concat([new, merged_row.to_frame().T], ignore_index=True)\n",
    "        \n",
    "    return new\n",
    "\n",
    "\n",
    "def delete_timestamp(df):\n",
    "    to_remove=[\"timestamp_max\",\"timestamp_min\",\"timestamp_mean\",\"timestamp_STD\"]\n",
    "    new= df.drop(columns=to_remove, axis=1, errors='ignore')\n",
    "\n",
    "    return new\n",
    "\n",
    "def update_name(df):\n",
    "    new_column_names = [ 'timestamp_max','acc_x_dashboard_l_max', 'acc_y_dashboard_l_max', \n",
    "        'acc_z_dashboard_l_max', 'speed_max', 'gyro_x_dashboard_l_max', \n",
    "        'gyro_y_dashboard_l_max', 'gyro_z_dashboard_l_max', \n",
    "        'timestamp_min', 'acc_x_dashboard_l_min', 'acc_y_dashboard_l_min', \n",
    "        'acc_z_dashboard_l_min', 'speed_min', 'gyro_x_dashboard_l_min', \n",
    "        'gyro_y_dashboard_l_min', 'gyro_z_dashboard_l_min', \n",
    "        'timestamp_mean', 'acc_x_dashboard_l_mean', 'acc_y_dashboard_l_mean', \n",
    "        'acc_z_dashboard_l_mean', 'speed_mean', 'gyro_x_dashboard_l_mean', \n",
    "        'gyro_y_dashboard_l_mean', 'gyro_z_dashboard_l_mean', \n",
    "        'timestamp_STD_', 'acc_x_dashboard_l_STD_', 'acc_y_dashboard_l_STD_', \n",
    "        'acc_z_dashboard_l_STD_', 'speed_STD_', 'gyro_x_dashboard_l_STD_', \n",
    "        'gyro_y_dashboard_l_STD_', 'gyro_z_dashboard_l_STD_',\n",
    "         'acc_x_dashboard_l_jerk', 'acc_y_dashboard_l_jerk', \n",
    "        'acc_z_dashboard_l_jerk', 'speed_jerk', 'gyro_x_dashboard_l_jerk', \n",
    "        'gyro_y_dashboard_l_jerk', 'gyro_z_dashboard_l_jerk'\n",
    "    ]\n",
    "    df.columns = new_column_names\n",
    "    return df\n",
    "\n",
    "\n",
    "#function for computing the rolling window and the aggregation functions, \n",
    "#--> returns a disctionary containing the 9 dataframes winodwed (still separated)\n",
    "def computeWindow(name,df, windowSize=100, show=False):\n",
    "    if show:\n",
    "        print(\"--------------------------\")\n",
    "        print(f\"computing dataset {name}\")\n",
    "    \n",
    "    #define empty dataframe  \n",
    "\n",
    "    X= df.iloc[:, :-1]\n",
    "    Y= df.iloc[:len(df)-windowSize+1, -1]\n",
    "    windowed_df=pd.DataFrame()\n",
    "\n",
    "    #define starting and ending index\n",
    "    for start_idx in range(len(df)-windowSize+1):\n",
    "        end_idx=start_idx+windowSize\n",
    "\n",
    "        #extract rows belonging to the window\n",
    "        window=X.iloc[start_idx:end_idx]\n",
    "\n",
    "        #take beginning timestamp and ending timestamp\n",
    "        start_timestamp=window.iloc[0,0]\n",
    "        end_timestamp=window.iloc[-1,0]\n",
    "\n",
    "        \n",
    "        #keep only sensor data\n",
    "        sensor_data_window=window.iloc[:,1:]\n",
    "\n",
    "        #compute metrics for the specific window\n",
    "        max_values=window.max()\n",
    "        min_values=window.min()\n",
    "        mean_values=window.mean()\n",
    "        std_values=window.std()\n",
    "        jerk_values=(sensor_data_window.iloc[-1]-sensor_data_window.iloc[0])/(end_timestamp-start_timestamp)\n",
    "\n",
    "        #concate them (place side by side)\n",
    "        new_row=pd.concat([max_values,min_values,mean_values,std_values,jerk_values])\n",
    "        new_row = new_row.to_frame().T \n",
    "\n",
    "        windowed_df = pd.concat([windowed_df, new_row], ignore_index=True)\n",
    "\n",
    "        \n",
    "        if show and start_idx%10000==0:\n",
    "            print(start_idx)\n",
    "\n",
    "    #place aside again  X and Y\n",
    "    final=pd.concat([delete_timestamp(update_name(windowed_df)),Y], axis=1)  \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing datasets\n",
    "filepaths_left={\n",
    "    \"pvs1_gps_mpu\" : r\"../archive/PVS 1/dataset_gps_mpu_left.csv\",\n",
    "    \"pvs2_gps_mpu\" : r\"../archive/PVS 2/dataset_gps_mpu_left.csv\",\n",
    "    \"pvs3_gps_mpu\" : r\"../archive/PVS 3/dataset_gps_mpu_left.csv\",\n",
    "    \"pvs4_gps_mpu\" : r\"../archive/PVS 4/dataset_gps_mpu_left.csv\",\n",
    "    \"pvs5_gps_mpu\" : r\"../archive/PVS 5/dataset_gps_mpu_left.csv\",\n",
    "    \"pvs6_gps_mpu\" : r\"../archive/PVS 6/dataset_gps_mpu_left.csv\",\n",
    "    \"pvs7_gps_mpu\" : r\"../archive/PVS 7/dataset_gps_mpu_left.csv\",\n",
    "    \"pvs8_gps_mpu\" : r\"../archive/PVS 8/dataset_gps_mpu_left.csv\",\n",
    "    \"pvs9_gps_mpu\" : r\"../archive/PVS 9/dataset_gps_mpu_left.csv\"\n",
    "}\n",
    "\n",
    "filepaths_right={\n",
    "    \"pvs1_gps_mpu\" : r\"../archive/PVS 1/dataset_gps_mpu_right.csv\",\n",
    "    \"pvs2_gps_mpu\" : r\"../archive/PVS 2/dataset_gps_mpu_right.csv\",\n",
    "    \"pvs3_gps_mpu\" : r\"../archive/PVS 3/dataset_gps_mpu_right.csv\",\n",
    "    \"pvs4_gps_mpu\" : r\"../archive/PVS 4/dataset_gps_mpu_right.csv\",\n",
    "    \"pvs5_gps_mpu\" : r\"../archive/PVS 5/dataset_gps_mpu_right.csv\",\n",
    "    \"pvs6_gps_mpu\" : r\"../archive/PVS 6/dataset_gps_mpu_right.csv\",\n",
    "    \"pvs7_gps_mpu\" : r\"../archive/PVS 7/dataset_gps_mpu_right.csv\",\n",
    "    \"pvs8_gps_mpu\" : r\"../archive/PVS 8/dataset_gps_mpu_right.csv\",\n",
    "    \"pvs9_gps_mpu\" : r\"../archive/PVS 9/dataset_gps_mpu_right.csv\"\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "# Left and right corresponds to sensordata from either side of the cars\n",
    "datasets_left = {name:pd.read_csv(path) for name,path in filepaths_left.items()}\n",
    "datasets_right = {name:pd.read_csv(path) for name,path in filepaths_right.items()}\n",
    "\n",
    "datasets = {name: merge(datasets_left[name],datasets_right[name], name) for name in datasets_right.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_rm_zeros = remove_zero_values(datasets)\n",
    "equalized_datasets = equalize_dfs(datasets_rm_zeros)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5384f283-a71a-4b09-875b-c0311064730e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing pvs1_gps_mpu\n",
      "Reducing pvs2_gps_mpu\n",
      "Reducing pvs3_gps_mpu\n",
      "Reducing pvs4_gps_mpu\n",
      "Reducing pvs5_gps_mpu\n",
      "Reducing pvs6_gps_mpu\n",
      "Reducing pvs7_gps_mpu\n",
      "Reducing pvs8_gps_mpu\n",
      "Reducing pvs9_gps_mpu\n"
     ]
    }
   ],
   "source": [
    "#REDUCE THE DATA\n",
    "ratio=40\n",
    "rounded_datasets = round_dfs(equalized_datasets, ratio)\n",
    "\n",
    "#for key, index in rounded_datasets.items():\n",
    "#    print(\"Length of speed list for\", key, \": \", len(rounded_datasets[key]['speed']))\n",
    "\n",
    "datasets_reduced={name:reduce_df(rounded_datasets[name],ratio,name) for name in rounded_datasets.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary with DataFrames\n",
    "with open(\"datasets_reduced.pkl\", \"wb\") as file:\n",
    "    pickle.dump(datasets_reduced, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "computing dataset pvs1_gps_mpu\n",
      "0\n",
      "--------------------------\n",
      "computing dataset pvs2_gps_mpu\n",
      "0\n",
      "--------------------------\n",
      "computing dataset pvs3_gps_mpu\n",
      "0\n",
      "--------------------------\n",
      "computing dataset pvs4_gps_mpu\n",
      "0\n",
      "--------------------------\n",
      "computing dataset pvs5_gps_mpu\n",
      "0\n",
      "--------------------------\n",
      "computing dataset pvs6_gps_mpu\n",
      "0\n",
      "--------------------------\n",
      "computing dataset pvs7_gps_mpu\n",
      "0\n",
      "--------------------------\n",
      "computing dataset pvs8_gps_mpu\n",
      "0\n",
      "--------------------------\n",
      "computing dataset pvs9_gps_mpu\n",
      "0\n",
      "--------------------------\n",
      "computing dataset pvs1_gps_mpu\n",
      "0\n",
      "--------------------------\n",
      "computing dataset pvs2_gps_mpu\n",
      "0\n",
      "--------------------------\n",
      "computing dataset pvs3_gps_mpu\n",
      "0\n",
      "--------------------------\n",
      "computing dataset pvs4_gps_mpu\n",
      "0\n",
      "--------------------------\n",
      "computing dataset pvs5_gps_mpu\n",
      "0\n",
      "--------------------------\n",
      "computing dataset pvs6_gps_mpu\n",
      "0\n",
      "--------------------------\n",
      "computing dataset pvs7_gps_mpu\n",
      "0\n",
      "--------------------------\n",
      "computing dataset pvs8_gps_mpu\n",
      "0\n",
      "--------------------------\n",
      "computing dataset pvs9_gps_mpu\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#compute all windowed dfs in order to store them : \n",
    "window_sizes=[100,200]\n",
    "for size in window_sizes:\n",
    "    dfs_windowed={name:computeWindow(name,datasets_reduced[name],size, True) for name in datasets_reduced.keys()}\n",
    "    # Save the dictionary with DataFrames\n",
    "    with open(f\"dfs_windowed_{size}.pkl\", \"wb\") as file:\n",
    "        pickle.dump(dfs_windowed, file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ada19aa6-4362-4986-8227-f6e3f36b6d66",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tuning_and_evaluation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m test_track\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[0;32m      3\u001b[0m threshold_pca\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtuning_and_evaluation\u001b[49m(datasets_reduced,window_sizes,model_type,num_folds,test_track,threshold_pca)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tuning_and_evaluation' is not defined"
     ]
    }
   ],
   "source": [
    "#create the full df\n",
    "test_track=3\n",
    "threshold_pca=0.6\n",
    "window_sizes=[100,200,300]\n",
    "model_type=\"lr\"\n",
    "num_folds=5\n",
    "\n",
    "tuning_and_evaluation(datasets_reduced,window_sizes,model_type,num_folds,test_track,threshold_pca)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
